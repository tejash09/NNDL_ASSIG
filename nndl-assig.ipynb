{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install datasets seqeval tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T14:58:15.029854Z","iopub.execute_input":"2025-04-18T14:58:15.030596Z","iopub.status.idle":"2025-04-18T14:58:22.046785Z","shell.execute_reply.started":"2025-04-18T14:58:15.030566Z","shell.execute_reply":"2025-04-18T14:58:22.046057Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nCollecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.2.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=8130d68b34f2b72d1158a5cb0d5a7d7d0d19ff15e6c2475048816501038569ae\n  Stored in directory: /root/.cache/pip/wheels/bc/92/f0/243288f899c2eacdfa8c5f9aede4c71a9bad0ee26a01dc5ead\nSuccessfully built seqeval\nInstalling collected packages: fsspec, seqeval\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.3.3.83 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.9.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.3.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.8.93 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.8.93 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2024.12.0 seqeval-1.2.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset\nfrom transformers import BertTokenizer, BertModel, logging, BertTokenizerFast\nfrom seqeval.metrics import f1_score, classification_report\nimport numpy as np\nfrom tqdm import tqdm\nimport warnings\n\n# Ignore specific warnings\nwarnings.filterwarnings(\"ignore\", message=\".*Unable to register.*\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", message=\".*Xet Storage.*\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", message=\".*You are using `torch.load`.*\", category=FutureWarning)\n\n\n# Reduce verbosity of transformers warnings\nlogging.set_verbosity_error()\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load dataset\n# Use trust_remote_code=True to avoid the prompt\ndataset = load_dataset(\"eriktks/conll2003\", trust_remote_code=True)\nprint(f\"Dataset loaded: {dataset.keys()}\")\n\n# --- Define NER tags with START/END ---\nSTART_TAG = \"<START>\"\nEND_TAG = \"<END>\"\ntag2idx = {\n    'O': 0,\n    'B-PER': 1, 'I-PER': 2,\n    'B-ORG': 3, 'I-ORG': 4,\n    'B-LOC': 5, 'I-LOC': 6,\n    'B-MISC': 7, 'I-MISC': 8,\n    START_TAG: 9, END_TAG: 10\n}\nidx2tag = {v: k for k, v in tag2idx.items()}\nnum_tags = len(tag2idx) # Now 11\nprint(f\"Number of tags (including START/END): {num_tags}\")\n\n# Load tokenizer and BERT model\ntry:\n    tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n    print(\"Using fast tokenizer\")\nexcept ImportError:\n    print(\"Fast tokenizer not available, using standard tokenizer\")\n    tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\nbert_model = BertModel.from_pretrained('bert-base-cased')\n\nclass NERDataset(Dataset):\n    def __init__(self, dataset_split, tokenizer, tag2idx, max_len=128):\n        self.dataset = dataset_split\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.tag2idx = tag2idx\n        # Use index 0 ('O') for unknown tags encountered\n        self.unknown_tag_idx = self.tag2idx['O']\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        words = self.dataset[idx]['tokens']\n        original_tags = self.dataset[idx]['ner_tags']\n\n        # Convert CoNLL numeric tags to string tags using the dataset's feature info\n        conll_tag_names = self.dataset.features['ner_tags'].feature.names\n        tags = [conll_tag_names[tag_idx] for tag_idx in original_tags]\n\n        # Map string tags to our tag indices\n        tag_ids = [self.tag2idx.get(tag, self.unknown_tag_idx) for tag in tags]\n\n        # Tokenize words using the tokenizer\n        # Important: Use is_split_into_words=True and get word_ids\n        encoding = self.tokenizer(\n            words,\n            is_split_into_words=True,\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_len,\n            return_offsets_mapping=False # Not needed here, word_ids is better\n        )\n\n        input_ids = encoding['input_ids']\n        attention_mask = encoding['attention_mask']\n\n        # Align tags with tokenized input_ids using word_ids\n        aligned_tags = [-100] * len(input_ids)  # Initialize with -100 (ignore index)\n        word_ids = encoding.word_ids()\n\n        previous_word_idx = None\n        for i, word_idx in enumerate(word_ids):\n            # Handle special tokens ([CLS], [SEP], [PAD]) which have word_idx = None\n            if word_idx is None:\n                aligned_tags[i] = -100\n            # Handle tokens corresponding to actual words\n            elif word_idx != previous_word_idx:\n                # Only label the first token of a given word\n                if word_idx < len(tag_ids):\n                     aligned_tags[i] = tag_ids[word_idx]\n                else:\n                     # Should not happen if word_ids aligns correctly with input words\n                     aligned_tags[i] = -100 # Or self.unknown_tag_idx if preferred, but -100 is standard\n                previous_word_idx = word_idx\n            else:\n                 # Label subsequent tokens of the same word with -100\n                 aligned_tags[i] = -100\n\n        return {\n            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n            'labels': torch.tensor(aligned_tags, dtype=torch.long)\n        }\n\n# --- Updated Model: BERT + BiLSTM + CRF with START/END tags ---\nclass BERT_BiLSTM_CRF(nn.Module):\n    def __init__(self, bert_model, lstm_hidden_dim, num_tags, tag2idx):\n        super(BERT_BiLSTM_CRF, self).__init__()\n        self.bert = bert_model\n        self.dropout = nn.Dropout(0.1)\n        self.lstm = nn.LSTM(\n            input_size=bert_model.config.hidden_size,\n            hidden_size=lstm_hidden_dim,\n            num_layers=1, # Reduced layers for simplicity, can be tuned\n            bidirectional=True,\n            batch_first=True\n        )\n        self.hidden2tag = nn.Linear(lstm_hidden_dim * 2, num_tags)\n\n        self.tag2idx = tag2idx\n        self.num_tags = num_tags\n        self.start_tag_idx = self.tag2idx[START_TAG]\n        self.end_tag_idx = self.tag2idx[END_TAG]\n\n        # CRF transitions parameter\n        self.transitions = nn.Parameter(torch.randn(num_tags, num_tags))\n\n        # Constrain transitions:\n        # - No transition *to* START_TAG\n        # - No transition *from* END_TAG\n        self.transitions.data[self.start_tag_idx, :] = -10000.0\n        self.transitions.data[:, self.end_tag_idx] = -10000.0\n        # Optional: Add BIO constraints later if needed\n\n    def _forward_alg(self, feats, mask):\n        batch_size, seq_len, tag_size = feats.size()\n        assert tag_size == self.num_tags\n\n        # Initialize alpha: log probability of being in state j at step 0\n        # Set START_TAG probability to 0 (-inf for others)\n        log_alpha = torch.full((batch_size, self.num_tags), -10000.0, device=device)\n        log_alpha[:, self.start_tag_idx] = 0.0\n\n        # Iterate through the sequence\n        for t in range(seq_len):\n            # Get emission scores for timestep t for all tags and batches\n            emit_scores_t = feats[:, t].unsqueeze(1)  # (batch_size, 1, num_tags)\n\n            # Get transition scores (independent of batch)\n            # trans_scores[i, j] = transition score from tag i to tag j\n            trans_scores = self.transitions.unsqueeze(0) # (1, num_tags, num_tags)\n\n            # Combine previous alpha, transition, and emission scores\n            # log_alpha has shape (batch_size, num_tags)\n            # alpha_t[j] = log P(path ending at state j at time t)\n            # next_alpha_t[k] = log P(path ending at state k at time t+1)\n            #                 = logsumexp_j (alpha_t[j] + transition[j, k] + emission[k])\n            log_alpha_t = log_alpha.unsqueeze(2) # (batch_size, num_tags, 1)\n            next_log_alpha_t = log_alpha_t + trans_scores + emit_scores_t # (batch_size, num_tags, num_tags)\n            next_log_alpha_t = torch.logsumexp(next_log_alpha_t, dim=1) # (batch_size, num_tags)\n\n            # Apply mask: If mask is 0, keep the previous log_alpha\n            mask_t = mask[:, t].unsqueeze(1).float() # (batch_size, 1)\n            log_alpha = mask_t * next_log_alpha_t + (1 - mask_t) * log_alpha\n\n        # Add final transition to END_TAG\n        # Note: We consider the transition to END_TAG *after* the last emission score\n        # has been incorporated into log_alpha at the final valid timestep.\n        # The current log_alpha holds scores for paths ending at the last token.\n        log_alpha += self.transitions[self.end_tag_idx, :].unsqueeze(0)\n\n        # Log-sum-exp over the final scores for all tags to get the partition function Z(x)\n        # This represents the log probability of all possible paths.\n        log_partition_function = torch.logsumexp(log_alpha, dim=1) # (batch_size,)\n        return log_partition_function\n\n    def _score_sentence(self, feats, tags, mask):\n        batch_size, seq_len = tags.size()\n        assert feats.size(0) == batch_size and feats.size(1) == seq_len\n\n        # Initialize score with transition from START_TAG to the first actual tag\n        start_tags = torch.full((batch_size,), self.start_tag_idx, dtype=torch.long, device=device)\n        # Get transition score from START to first tag (tags[:, 0])\n        score = self.transitions[start_tags, tags[:, 0]]\n\n        # Add emission score for the first tag, only if it's not masked\n        score += torch.gather(feats[:, 0], 1, tags[:, 0].unsqueeze(1)).squeeze(1) * mask[:, 0].float()\n\n        # Iterate through the rest of the sequence (from t=1 to seq_len-1)\n        for t in range(1, seq_len):\n            mask_t = mask[:, t].float()\n            # Transition score from previous tag (tags[:, t-1]) to current tag (tags[:, t])\n            trans_score = self.transitions[tags[:, t-1], tags[:, t]]\n            # Emission score for the current tag (tags[:, t]) at timestep t\n            emit_score = torch.gather(feats[:, t], 1, tags[:, t].unsqueeze(1)).squeeze(1)\n            # Add scores only for non-masked positions\n            score += mask_t * (trans_score + emit_score)\n\n        # Add transition score to END_TAG from the last valid tag in the sequence\n        # Find the index of the last valid token for each sequence in the batch\n        last_valid_idx = mask.sum(dim=1).long() - 1 # (batch_size,)\n        # Get the tag at the last valid index for each sequence\n        # Need to handle cases where seq len is 0 (all masked) -> last_valid_idx = -1\n        valid_seq_mask = last_valid_idx >= 0\n        if valid_seq_mask.any():\n            last_valid_tags = torch.gather(tags[valid_seq_mask], 1, last_valid_idx[valid_seq_mask].unsqueeze(1)).squeeze(1)\n            # Add transition score from last_valid_tag to END_TAG\n            score[valid_seq_mask] += self.transitions[last_valid_tags, self.end_tag_idx]\n\n        return score\n\n    def _viterbi_decode(self, feats, mask):\n        batch_size, seq_len, tag_size = feats.size()\n        assert tag_size == self.num_tags\n\n        # Initialize Viterbi path scores (log probabilities)\n        # Set START_TAG score to 0, others to -inf\n        log_delta = torch.full((batch_size, self.num_tags), -10000.0, device=device)\n        log_delta[:, self.start_tag_idx] = 0.0\n\n        # Initialize backpointers matrix (to store the best previous tag index)\n        psi = torch.zeros(batch_size, seq_len, self.num_tags, dtype=torch.long, device=device)\n\n        # Iterate through the sequence\n        for t in range(seq_len):\n            # Get emission scores for timestep t\n            emit_scores_t = feats[:, t].unsqueeze(1) # (batch_size, 1, num_tags)\n\n            # Get transition scores\n            trans_scores = self.transitions.unsqueeze(0) # (1, num_tags, num_tags)\n\n            # Combine previous delta, transition, and emission scores\n            # log_delta_t has shape (batch_size, num_tags)\n            # log_delta_t[j] = max log P(best path ending at state j at time t)\n            # next_log_delta_t[k] = max_j (log_delta_t[j] + transition[j, k] + emission[k])\n            log_delta_t = log_delta.unsqueeze(2) # (batch_size, num_tags, 1)\n            next_log_delta_t = log_delta_t + trans_scores + emit_scores_t # (batch_size, num_tags, num_tags)\n\n            # Find the maximum score and the corresponding previous tag index (backpointer)\n            max_log_delta_t, psi[:, t] = torch.max(next_log_delta_t, dim=1) # (batch_size, num_tags), (batch_size, num_tags)\n\n            # Apply mask: If mask is 0, keep the previous log_delta\n            mask_t = mask[:, t].unsqueeze(1).float() # (batch_size, 1)\n            log_delta = mask_t * max_log_delta_t + (1 - mask_t) * log_delta\n\n        # Add final transition to END_TAG\n        log_delta += self.transitions[self.end_tag_idx, :].unsqueeze(0)\n\n        # Find the best score and the tag index at the end of the sequence (before END transition)\n        # The best score should correspond to the path ending in END_TAG, but we find the max over all tags\n        # The tag index corresponds to the tag at the *last valid position* that leads to the best overall score.\n        best_path_score, last_tag = torch.max(log_delta, dim=1) # (batch_size,), (batch_size,)\n\n        # Backtrack using backpointers (psi) to find the best path\n        best_paths = torch.zeros(batch_size, seq_len, dtype=torch.long, device=device)\n        seq_ends = mask.sum(dim=1).long() # Length of each sequence\n\n        for b in range(batch_size):\n            # Start backtracking from the last valid position\n            seq_end = seq_ends[b].item()\n            if seq_end == 0: # Handle empty sequences after masking\n                continue\n\n            # The 'last_tag' is the best tag index at the last valid timestep (seq_end - 1)\n            best_tag_b = last_tag[b].item()\n            best_paths[b, seq_end - 1] = best_tag_b\n\n            # Follow backpointers from t = seq_end - 1 down to 0\n            for t in range(seq_end - 2, -1, -1):\n                best_tag_b = psi[b, t + 1, best_tag_b].item()\n                best_paths[b, t] = best_tag_b\n\n        return best_path_score, best_paths\n\n    def neg_log_likelihood(self, feats, tags, mask):\n        # Calculate the log partition function (sum over all paths)\n        forward_score = self._forward_alg(feats, mask)\n        # Calculate the score of the true path\n        gold_score = self._score_sentence(feats, tags, mask)\n\n        # Negative log likelihood = log Z(x) - score(true_path)\n        # Average over the batch\n        return torch.mean(forward_score - gold_score)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        # Get BERT embeddings\n        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        sequence_output = bert_outputs.last_hidden_state\n        sequence_output = self.dropout(sequence_output)\n\n        # Pass through BiLSTM\n        lstm_output, _ = self.lstm(sequence_output)\n        lstm_output = self.dropout(lstm_output)\n\n        # Project to tag space (get emission scores)\n        emissions = self.hidden2tag(lstm_output) # (batch, seq_len, num_tags)\n\n        if labels is not None:\n            # --- Training/Validation ---\n            # Create mask for loss calculation: ignore padding AND -100 labels\n            # Mask should be 1 for valid tokens, 0 otherwise\n            loss_mask = (attention_mask == 1) & (labels != -100)\n            loss_mask = loss_mask.long() # Convert boolean mask to long/float for CRF methods\n\n            # Clone labels and replace -100 with a valid index (e.g., 'O')\n            # This index won't contribute to the loss because of the mask,\n            # but CRF methods expect valid indices.\n            valid_labels = labels.clone()\n            valid_labels[labels == -100] = self.tag2idx['O'] # Replace -100 with 'O' index\n\n            # Calculate negative log likelihood loss using the mask\n            loss = self.neg_log_likelihood(emissions, valid_labels, loss_mask)\n\n            # Decode best path using the same mask (for potential inspection during training)\n            # Note: Decoding during training adds overhead. Usually only done for evaluation.\n            # We return it here as the original code did.\n            _, best_path = self._viterbi_decode(emissions, loss_mask)\n\n            return loss, best_path\n        else:\n            # --- Inference ---\n            # Use attention mask directly for decoding (assumes no -100 labels in input)\n            # Mask should be 1 for non-padding tokens, 0 for padding\n            inference_mask = attention_mask.long()\n            _, best_path = self._viterbi_decode(emissions, inference_mask)\n            return best_path\n\ndef prepare_datasets(tag2idx):\n    # Prepare datasets\n    train_dataset = NERDataset(dataset['train'], tokenizer, tag2idx)\n    val_dataset = NERDataset(dataset['validation'], tokenizer, tag2idx)\n    test_dataset = NERDataset(dataset['test'], tokenizer, tag2idx)\n\n    # Create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True) # Reduced batch size\n    val_loader = DataLoader(val_dataset, batch_size=16)\n    test_loader = DataLoader(test_dataset, batch_size=16)\n\n    return train_loader, val_loader, test_loader\n\ndef train_model(model, train_loader, val_loader, epochs=4, learning_rate=3e-5): # Adjusted LR and epochs\n    # Prepare optimizer\n    # Separate parameters for BERT and other layers if desired (differential learning rates)\n    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n    # Scheduler (optional, but often helpful)\n    # Example: Linear warmup and decay\n    # num_training_steps = len(train_loader) * epochs\n    # scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9) # Simpler scheduler\n\n    best_f1 = 0.0\n    model.to(device)\n\n    for epoch in range(epochs):\n        # Training\n        model.train()\n        total_loss = 0\n\n        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{epochs}\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n\n            # Forward pass\n            optimizer.zero_grad()\n            loss, _ = model(input_ids, attention_mask, labels=labels) # Pass labels\n\n            # Backward pass\n            # Check for NaN/inf loss\n            if torch.isnan(loss) or torch.isinf(loss):\n                print(f\"Warning: NaN or Inf loss detected: {loss.item()}. Skipping batch.\")\n                torch.cuda.empty_cache() # Clear cache if memory issues might be related\n                continue\n\n            loss.backward()\n            # Gradient clipping (optional but recommended for stability)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            # scheduler.step() # Step scheduler per batch if using linear warmup/decay\n\n            total_loss += loss.item()\n\n        avg_loss = total_loss / len(train_loader) if len(train_loader) > 0 else 0\n        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {avg_loss:.4f}\")\n\n        # Validation\n        val_f1 = evaluate(model, val_loader, idx2tag) # Pass idx2tag\n        print(f\"Epoch {epoch+1}/{epochs}, Validation F1: {val_f1:.4f}\")\n\n        # Save best model based on validation F1\n        if val_f1 > best_f1:\n            best_f1 = val_f1\n            print(f\"New best validation F1: {best_f1:.4f}. Saving model...\")\n            torch.save(model.state_dict(), 'best_ner_model.pt')\n\n        scheduler.step() # Step scheduler per epoch if using StepLR\n\n    print(f\"Training finished. Best Validation F1: {best_f1:.4f}\")\n    return model\n\ndef evaluate(model, data_loader, idx2tag_map):\n    model.eval()\n    all_predictions = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device) # Ground truth labels\n\n            # Get predictions from the model (forward pass without labels)\n            best_paths = model(input_ids, attention_mask) # Returns predicted tag indices\n\n            # Process batch results\n            for i in range(input_ids.size(0)):\n                # Create mask to select valid tokens (not padding, not ignored -100 labels)\n                valid_mask = (attention_mask[i] == 1) & (labels[i] != -100)\n\n                # Get predicted tags for valid tokens\n                valid_preds = best_paths[i][valid_mask]\n                # Get ground truth tags for valid tokens\n                valid_labels = labels[i][valid_mask]\n\n                # Convert indices to tag names, handling potential out-of-bounds\n                pred_tags = [idx2tag_map.get(tag_idx.item(), 'O') for tag_idx in valid_preds]\n                gold_tags = [idx2tag_map.get(tag_idx.item(), 'O') for tag_idx in valid_labels]\n\n                # Add the sequence tags to the overall lists\n                # Ensure we don't add empty lists if a sequence had no valid tokens\n                if pred_tags and gold_tags:\n                    all_predictions.append(pred_tags)\n                    all_labels.append(gold_tags)\n\n    # Check if we collected any results\n    if not all_labels or not all_predictions:\n        print(\"Warning: No valid labels or predictions found during evaluation.\")\n        return 0.0\n\n    try:\n        # Calculate F1 score using seqeval\n        # Ensure labels are in the correct BIO format if needed by seqeval (they should be)\n        f1 = f1_score(all_labels, all_predictions, average='macro') # Use macro F1 for overall performance\n        print(\"\\nClassification Report (Validation/Test):\")\n        print(classification_report(all_labels, all_predictions, digits=4))\n        return f1\n    except Exception as e:\n        print(f\"Error calculating F1 score: {e}\")\n        # Print sample data for debugging if error occurs\n        print(\"Sample Gold:\", all_labels[0][:20] if all_labels else \"N/A\")\n        print(\"Sample Pred:\", all_predictions[0][:20] if all_predictions else \"N/A\")\n        return 0.0\n\n\ndef main():\n    # Set seeds for reproducibility\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n\n    # Prepare datasets using the updated tag mapping\n    train_loader, val_loader, test_loader = prepare_datasets(tag2idx)\n    \n    # Initialize model with updated parameters\n    model = BERT_BiLSTM_CRF(\n        bert_model,\n        lstm_hidden_dim=256,\n        num_tags=num_tags, # Pass the updated num_tags (including START/END)\n        tag2idx=tag2idx     # Pass the tag mapping\n    )\n\n    # Train model with potentially adjusted hyperparameters\n    print(\"Starting training...\")\n    model = train_model(model, train_loader, val_loader, epochs=3, learning_rate=3e-5) # Reduced epochs, common LR\n\n    # Load best model saved during training for final evaluation\n    print(\"\\nLoading best model for final test evaluation...\")\n    model.load_state_dict(torch.load('best_ner_model.pt'))\n    model.to(device) # Ensure model is on the correct device after loading\n\n    # Evaluate on the test set\n    print(\"Evaluating on Test Set...\")\n    test_f1 = evaluate(model, test_loader, idx2tag) # Pass idx2tag\n    print(f\"\\nFinal Test F1 Score (Macro): {test_f1:.4f}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T15:02:58.417443Z","iopub.execute_input":"2025-04-18T15:02:58.418082Z","iopub.status.idle":"2025-04-18T15:19:59.826457Z","shell.execute_reply.started":"2025-04-18T15:02:58.418060Z","shell.execute_reply":"2025-04-18T15:19:59.825662Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nDataset loaded: dict_keys(['train', 'validation', 'test'])\nNumber of tags (including START/END): 11\nUsing fast tokenizer\nStarting training...\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1/3: 100%|██████████| 878/878 [05:10<00:00,  2.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3, Training Loss: 10003.9656\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 204/204 [00:19<00:00, 10.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Validation/Test):\n              precision    recall  f1-score   support\n\n         LOC     0.9706    0.9341    0.9520      1837\n        MISC     0.8596    0.8633    0.8615       922\n         ORG     0.8783    0.9038    0.8908      1341\n         PER     0.9420    0.9739    0.9577      1836\n\n   micro avg     0.9230    0.9286    0.9258      5936\n   macro avg     0.9126    0.9188    0.9155      5936\nweighted avg     0.9237    0.9286    0.9259      5936\n\nEpoch 1/3, Validation F1: 0.9155\nNew best validation F1: 0.9155. Saving model...\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 2/3: 100%|██████████| 878/878 [05:11<00:00,  2.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/3, Training Loss: 9997.2098\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 204/204 [00:19<00:00, 10.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Validation/Test):\n              precision    recall  f1-score   support\n\n         LOC     0.9712    0.9352    0.9529      1837\n        MISC     0.8720    0.9089    0.8901       922\n         ORG     0.8611    0.9336    0.8959      1341\n         PER     0.9762    0.9602    0.9681      1836\n\n   micro avg     0.9301    0.9385    0.9343      5936\n   macro avg     0.9201    0.9345    0.9267      5936\nweighted avg     0.9324    0.9385    0.9350      5936\n\nEpoch 2/3, Validation F1: 0.9267\nNew best validation F1: 0.9267. Saving model...\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 3/3: 100%|██████████| 878/878 [05:12<00:00,  2.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/3, Training Loss: 9994.4451\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 204/204 [00:19<00:00, 10.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Validation/Test):\n              precision    recall  f1-score   support\n\n         LOC     0.9643    0.9559    0.9601      1837\n        MISC     0.8818    0.9143    0.8978       922\n         ORG     0.8789    0.9366    0.9069      1341\n         PER     0.9777    0.9542    0.9658      1836\n\n   micro avg     0.9348    0.9446    0.9397      5936\n   macro avg     0.9257    0.9403    0.9326      5936\nweighted avg     0.9363    0.9446    0.9402      5936\n\nEpoch 3/3, Validation F1: 0.9326\nNew best validation F1: 0.9326. Saving model...\nTraining finished. Best Validation F1: 0.9326\n\nLoading best model for final test evaluation...\nEvaluating on Test Set...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 216/216 [00:20<00:00, 10.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report (Validation/Test):\n              precision    recall  f1-score   support\n\n         LOC     0.9272    0.9178    0.9225      1666\n        MISC     0.7445    0.8177    0.7794       702\n         ORG     0.8403    0.9121    0.8747      1661\n         PER     0.9734    0.9276    0.9499      1615\n\n   micro avg     0.8879    0.9064    0.8971      5644\n   macro avg     0.8713    0.8938    0.8816      5644\nweighted avg     0.8921    0.9064    0.8985      5644\n\n\nFinal Test F1 Score (Macro): 0.8816\n","output_type":"stream"}],"execution_count":3}]}